{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d8d6fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2230ef78",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e43868fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Data/Final_Data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a2d01e",
   "metadata": {},
   "source": [
    "# Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45340486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/seanprajs1/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/seanprajs1/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Functions\n",
    "    #Hashtag Extraction\n",
    "import re\n",
    "    #Topic Modelling\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "from itertools import chain\n",
    "import string\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dd1eb3",
   "metadata": {},
   "source": [
    "# Build Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "136ea69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hashtags\n",
    "hashtags = re.compile('#\\w*') #search only for words that come after hashtags\n",
    "\n",
    "#Web Links\n",
    "links = re.compile('http\\S+') #define links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d94d22a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to Extract Hashtags and Key Words from message\n",
    "def extract_hashtags(df,column):\n",
    "    hashtaglist = [hashtags.findall(str(i)) for i in df[column]] #extract hashtags\n",
    "    return hashtaglist\n",
    "\n",
    "#Pre-Processing and Cleaning of Text\n",
    "stopwordlist = set(stopwords.words('english')) #stopwords\n",
    "punctuation = set(string.punctuation) #list of common punctuation\n",
    "lemmatize = WordNetLemmatizer() #create lemmatizing function\n",
    "stem = PorterStemmer() #create stemming function\n",
    "\n",
    "#function to simplify tagging of words the way they are used in sentences\n",
    "#function from GeeksForGeeks\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#lemmatizing function\n",
    "def word_lemmatizer(string):\n",
    "    #p = re.compile('#\\w*') #define hashtags\n",
    "    #string = re.sub(p,'',string) #remove hashtags from sentence\n",
    "    string = re.sub(links,'',string) #remove links from sentence    \n",
    "    string = \" \".join([i for i in string.lower().split() if i not in stopwordlist]) #remove stopwords\n",
    "    string = ''.join(ch for ch in string if ch not in punctuation) #remove punctuation\n",
    "\n",
    "    tagged_string = nltk.pos_tag(nltk.word_tokenize(string)) #tag each word in the string with how it is used in the sentence (verb, noun, etc.)\n",
    "    retagged_string = set(map(lambda x: (x[0], pos_tagger(x[1])), tagged_string)) #use simplified tags\n",
    "    #following function derived from GeeksForGeeks\n",
    "    lemmatized_words = set(word if tag is None else lemmatize.lemmatize(word,tag) for word,tag in retagged_string)\n",
    "    lemmatized_string = \" \".join(lemmatized_words) #recombine list of words into message\n",
    "    return lemmatized_string\n",
    "\n",
    "\n",
    "#topic modeling function\n",
    "def topic_modeler(strings,num_topics,num_passes,num_words):\n",
    "    lda = gensim.models.ldamodel.LdaModel #Define Latent Dirichlet Allocation Model\n",
    "    strings = [word_lemmatizer(string) for string in strings] #lemmatize messaGES\n",
    "    strings = [string.split() for string in strings] #split each message up into individual words\n",
    "    dct = corpora.Dictionary(strings) #create corpus\n",
    "    matrix = [dct.doc2bow(doc) for doc in strings] #creating a document term matrix\n",
    "    ldamodel = lda(matrix, num_topics=num_topics, id2word = dct, passes=num_passes)\n",
    "    return ldamodel.print_topics(num_topics=num_topics, num_words=num_words)\n",
    "\n",
    "#get all lemmas of word\n",
    "def fetch_lemmas(word):\n",
    "    word = word.replace(\" \",\"_\")#replace space with underscore to allow python to understand the word better\n",
    "    pos_tags = pos_tag([word]) #how is the word used in a sentence\n",
    "    pos_retagged = [(word,pos_tagger(tag)) for (word,tag) in pos_tags] #simplified tagging\n",
    "    lemmas = set(word if tag is None else lemmatize.lemmatize(word,tag) for word,tag in pos_retagged) #extract lemmatized words\n",
    "    lemmas = set(word.replace(\"_\",\" \") for word in lemmas) #convert phrases back to using space instead of underscore\n",
    "    synsets = wordnet.synsets(word) #find words with similar meanings\n",
    "    all_lemmas = set(chain.from_iterable([synset.lemma_names() for synset in synsets]))\n",
    "    all_lemmas = set(lemma.replace(\"_\",\" \") for lemma in all_lemmas)#convert phrases back to using space instead of underscore\n",
    "    return set(lemmas), all_lemmas\n",
    "\"\"\"note for later, include lemmas for the separate words in phrases as well\"\"\"\n",
    "\n",
    "#extract a certain word form a string\n",
    "def word_compiler(word):\n",
    "    wordre = re.compile(fr'\\b{word}\\b',re.IGNORECASE)\n",
    "    return wordre\n",
    "\n",
    "def word_extracter_index(stringlist, word):\n",
    "    wordre = word_compiler(word)\n",
    "    stringswithkeyword = list((i,wordre.findall(stringlist[i]))  if isinstance(stringlist[i],str) \n",
    "                              else (i,[]) for i in range(len(stringlist)))\n",
    "    stringswithkeyword = list((index,word) for (index,word) in stringswithkeyword if word != [])\n",
    "    stringswithkeyword = list(index for (index,word) in stringswithkeyword)\n",
    "    return stringswithkeyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49d59365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hashtag remover\n",
    "def remove_hashtag_in_list_of_strings(lst):\n",
    "    return [item.lstrip('#') for item in lst]\n",
    "\n",
    "def most_popular_hashtags_by_topic(df,column):\n",
    "    topic = input('Please put in the topic of your interest: ')\n",
    "    #topic_stem = stem.stem(topic)\n",
    "    topic_tokens, topic_synonyms = fetch_lemmas(topic)\n",
    "    allsearchwords = topic_tokens | topic_synonyms\n",
    "    #search through column for rows with these words inside\n",
    "    stringswithkeywords = [word_extracter_index(df[column],word) \n",
    "                           for word in allsearchwords]#find indices for all words\n",
    "    indexlist = sorted(list(set([item \n",
    "                                           for sublist in stringswithkeywords for item in sublist\n",
    "                                          ])))#extrapolate all lists to a single list and remove duplicates\n",
    "    #search for all hashtags in those rows\n",
    "    relevantrows = df.iloc[indexlist].copy()\n",
    "    relevantrows = relevantrows.reset_index(drop=True)\n",
    "    #extract hashtags\n",
    "    relevantrows['Hashtags'] = extract_hashtags(relevantrows,column)\n",
    "    #drop rows that do not have hashtags\n",
    "    relevantrows = relevantrows[relevantrows.Hashtags.apply(len) > 0]\n",
    "    #remove hashtags for easier handling\n",
    "    #relevantrows.Hashtags = relevantrows.Hashtags.apply(remove_hashtag_in_list_of_strings)\n",
    "    #lemmatize hashtags\n",
    "    #df.Hashtags = df.Hashtags.apply(lambda x: [])\n",
    "    #rank hashtags\n",
    "    hashtaglist = [item for sublist in relevantrows.Hashtags for item in sublist] #flatten list\n",
    "    counter = Counter(hashtaglist) #count occurences of hashtags\n",
    "    hashtagdf = pd.DataFrame(counter.items(),columns=['Hashtags','Number_Of_Occurences'])#create df with hashtags and no of occurences\n",
    "    hashtagdf_sorted = hashtagdf.sort_values('Number_Of_Occurences', ascending=False).reset_index(drop=True) #sort by number of occurences\n",
    "    #remove empty hashtags\n",
    "    hashtagdf_sorted = hashtagdf_sorted[hashtagdf_sorted[\"Hashtags\"] != \"#\"].reset_index(drop=True)\n",
    "    return hashtagdf_sorted\n",
    "\n",
    "def most_popular_hashtags_by_topic_print(df,column):\n",
    "    hashtagdf = most_popular_hashtags_by_topic(df,column)\n",
    "    if hashtagdf.shape[0] == 0:\n",
    "        print(\"There are no Hashtags associated with your topic\")\n",
    "    elif hashtagdf.shape[0] == 1:\n",
    "        print(f'The most popular Hashtags for your chosen topic are:')\n",
    "        print(f'1. {hashtagdf.Hashtags[0]}')\n",
    "    elif hashtagdf.shape[0] == 2:\n",
    "        print(f'The most popular Hashtags for your chosen topic are:')\n",
    "        print(f'1. {hashtagdf.Hashtags[0]}')\n",
    "        print(f'2. {hashtagdf.Hashtags[1]}')\n",
    "    elif hashtagdf.shape[0] == 3:\n",
    "        print(f'The most popular Hashtags for your chosen topic are:')\n",
    "        print(f'1. {hashtagdf.Hashtags[0]}')\n",
    "        print(f'2. {hashtagdf.Hashtags[1]}')\n",
    "        print(f'3. {hashtagdf.Hashtags[2]}')\n",
    "    elif hashtagdf.shape[0] == 4:\n",
    "        print(f'The most popular Hashtags for your chosen topic are:')\n",
    "        print(f'1. {hashtagdf.Hashtags[0]}')\n",
    "        print(f'2. {hashtagdf.Hashtags[1]}')\n",
    "        print(f'3. {hashtagdf.Hashtags[2]}')\n",
    "        print(f'4. {hashtagdf.Hashtags[3]}')\n",
    "    else:\n",
    "        print(f'The most popular Hashtags for your chosen topic are:')\n",
    "        print(f'1. {hashtagdf.Hashtags[0]}')\n",
    "        print(f'2. {hashtagdf.Hashtags[1]}')\n",
    "        print(f'3. {hashtagdf.Hashtags[2]}')\n",
    "        print(f'4. {hashtagdf.Hashtags[3]}')\n",
    "        print(f'5. {hashtagdf.Hashtags[4]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f184121",
   "metadata": {},
   "source": [
    "# Run Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f64aaee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please put in the topic of your interest: Beauty\n",
      "The most popular Hashtags for your chosen topic are:\n",
      "1. #beauty\n",
      "2. #fashion\n",
      "3. #makeup\n",
      "4. #love\n",
      "5. #style\n"
     ]
    }
   ],
   "source": [
    "most_popular_hashtags_by_topic_print(data,'Captions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3b1d2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please put in the topic of your interest: Gucci\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Number_Of_Occurences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#gucci</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#fashion</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#style</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#ootd</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#blogger</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>#explore</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>#exploreyourcity</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>#downtownsanfran</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>#california</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>#muchneeded</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>826 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Hashtags  Number_Of_Occurences\n",
       "0              #gucci                    42\n",
       "1            #fashion                    21\n",
       "2              #style                    17\n",
       "3               #ootd                    16\n",
       "4            #blogger                    13\n",
       "..                ...                   ...\n",
       "821          #explore                     1\n",
       "822  #exploreyourcity                     1\n",
       "823  #downtownsanfran                     1\n",
       "824       #california                     1\n",
       "825       #muchneeded                     1\n",
       "\n",
       "[826 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_popular_hashtags_by_topic(data,'Captions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab9789d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
